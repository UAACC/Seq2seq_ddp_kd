09/01/2024 16:01:01 - INFO - __main__ - ***** Running training *****
09/01/2024 16:01:01 - INFO - __main__ -   Num examples = 204045
09/01/2024 16:01:01 - INFO - __main__ -   Num Epochs = 1
09/01/2024 16:01:01 - INFO - __main__ -   Instantaneous batch size per device = 4
09/01/2024 16:01:01 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
09/01/2024 16:01:01 - INFO - __main__ -   Gradient Accumulation steps = 1
09/01/2024 16:01:01 - INFO - __main__ -   Total optimization steps = 12753.0
batch contains {'input_ids': tensor([[21603,    10,    37,  ...,    68,  1835,     1],
        [21603,    10,   299,  ...,  3024,    24,     1],
        [21603,    10,    37,  ...,     0,     0,     0],
        [21603,    10, 29784,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([[    3, 18540,  2487,   969,    29,    65,  8151,     8,   166,  7155,
             7, 31430,    13,   112, 16117,  2843,    38,  1401,    13,   112,
          5220,     7, 12191,    12,  2902,   581,     8,   789,    31,     7,
           126,  1456,  2219,     5,     1,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],
        [   94,    19,     3,     9,  8899,  5016,    24,  3200,   544,  1163,
            31,     7,  7353,  6123,  4889,    11,    80,    13,     8,   296,
            31,     7,  1374, 11441,    21,     8,  1058,    13,  1580,  1124,
             5,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],
        [   37,  2384,   636,    41, 13336,    61,   871,    33,   271, 16524,
            15,    26,    21,  6585,  1041,    16,     3,     9,  7358,   147,
           613,  8467,    11,     8, 12493,    13,  3518,  6036,     5,     1,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],
        [   71,  1268, 12039,    49,    65,  1869,   160,   804,   239,    44,
             8,  6885,   255,    65,  1279,    16,    21,     8,   336,  9455,
           203,     5,     1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]],
       device='cuda:0')}
LLM Loss: 3.532593011856079
teacher logits size torch.Size([4, 100, 32128])
teacher logits size torch.Size([400, 32128])
Teacher Logits: tensor([[-2.2929e+01,  4.7603e-02, -4.0989e+00,  ..., -2.7157e+01,
         -2.7750e+01, -2.5865e+01],
        [-6.3789e+01, -1.2040e+01, -1.0952e+01,  ..., -6.6149e+01,
         -7.2707e+01, -7.0956e+01],
        [-1.2277e+02, -3.2858e+01, -3.5927e+01,  ..., -1.2583e+02,
         -1.4082e+02, -1.3687e+02],
        ...,
        [-5.9584e+01, -1.0768e+01, -1.5097e+01,  ..., -6.3057e+01,
         -6.9555e+01, -6.6819e+01],
        [-5.0828e+01, -8.7129e+00, -1.4005e+01,  ..., -5.3955e+01,
         -5.9184e+01, -5.7100e+01],
        [-5.4257e+01, -8.3876e+00, -1.4835e+01,  ..., -5.7825e+01,
         -6.3888e+01, -6.0705e+01]], device='cuda:0')
Teacher Logits min: -140.81507873535156, max: 14.627083778381348
KL Divergence Loss: 0.7329944372177124
loss is 7.79818058013916
  0%|                                                                                                                                              | 0/12753 [00:00<?, ?it/s]